{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autotime\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "random.seed(1)\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 477 µs\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4, 5, 6, 7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.26 ms\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "log_format = '%(levelname)s %(asctime)s - %(message)s'\n",
    "logging.basicConfig(filename = '../logs/original_fsl.logs',\n",
    "                    level = logging.INFO,\n",
    "                    format = log_format,\n",
    "                    filemode = 'w')\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(368945, 19)\n",
      "Index(['subject_id', 'image_path', 'image_name', 'study_id', 'split',\n",
      "       'Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema',\n",
      "       'Enlarged Cardiomediastinum', 'Fracture', 'Lung Lesion', 'Lung Opacity',\n",
      "       'No Finding', 'Pleural Effusion', 'Pleural Other', 'Pneumonia',\n",
      "       'Pneumothorax', 'Support Devices'],\n",
      "      dtype='object')\n",
      "time: 2.13 s\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('../data/train_data.csv')\n",
    "print (train_data.shape)\n",
    "print (train_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>image_path</th>\n",
       "      <th>image_name</th>\n",
       "      <th>study_id</th>\n",
       "      <th>split</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Enlarged Cardiomediastinum</th>\n",
       "      <th>Fracture</th>\n",
       "      <th>Lung Lesion</th>\n",
       "      <th>Lung Opacity</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Pleural Effusion</th>\n",
       "      <th>Pleural Other</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Support Devices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s52769454</td>\n",
       "      <td>../../DataCenter/MIMIC-CXR/files/p18/p18190098...</td>\n",
       "      <td>8bf006fa-7169cd83-c30e7055-b109468a-2223477d</td>\n",
       "      <td>52769454</td>\n",
       "      <td>train</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s50754262</td>\n",
       "      <td>../../DataCenter/MIMIC-CXR/files/p18/p18190098...</td>\n",
       "      <td>48fbe534-50750d68-5afd36c2-e7aaf316-a31fcb66</td>\n",
       "      <td>50754262</td>\n",
       "      <td>train</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s51845898</td>\n",
       "      <td>../../DataCenter/MIMIC-CXR/files/p18/p18190098...</td>\n",
       "      <td>fc2e907b-c19b5434-cc3d4205-8d66ab01-a0dcc274</td>\n",
       "      <td>51845898</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s51845898</td>\n",
       "      <td>../../DataCenter/MIMIC-CXR/files/p18/p18190098...</td>\n",
       "      <td>e921e838-0c4b1f64-0572971d-b67b4730-4182681f</td>\n",
       "      <td>51845898</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s52846638</td>\n",
       "      <td>../../DataCenter/MIMIC-CXR/files/p18/p18190098...</td>\n",
       "      <td>c458d43d-f4a7eb7d-a51b95a8-f4aa2c82-52d243ff</td>\n",
       "      <td>52846638</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subject_id                                         image_path  \\\n",
       "0  s52769454  ../../DataCenter/MIMIC-CXR/files/p18/p18190098...   \n",
       "1  s50754262  ../../DataCenter/MIMIC-CXR/files/p18/p18190098...   \n",
       "2  s51845898  ../../DataCenter/MIMIC-CXR/files/p18/p18190098...   \n",
       "3  s51845898  ../../DataCenter/MIMIC-CXR/files/p18/p18190098...   \n",
       "4  s52846638  ../../DataCenter/MIMIC-CXR/files/p18/p18190098...   \n",
       "\n",
       "                                     image_name  study_id  split  Atelectasis  \\\n",
       "0  8bf006fa-7169cd83-c30e7055-b109468a-2223477d  52769454  train          0.0   \n",
       "1  48fbe534-50750d68-5afd36c2-e7aaf316-a31fcb66  50754262  train          0.0   \n",
       "2  fc2e907b-c19b5434-cc3d4205-8d66ab01-a0dcc274  51845898  train          1.0   \n",
       "3  e921e838-0c4b1f64-0572971d-b67b4730-4182681f  51845898  train          1.0   \n",
       "4  c458d43d-f4a7eb7d-a51b95a8-f4aa2c82-52d243ff  52846638  train          1.0   \n",
       "\n",
       "   Cardiomegaly  Consolidation  Edema  Enlarged Cardiomediastinum  Fracture  \\\n",
       "0           0.0            0.0    0.0                         0.0       0.0   \n",
       "1           0.0            0.0    0.0                         0.0       0.0   \n",
       "2           0.0            0.0    0.0                         0.0       0.0   \n",
       "3           0.0            0.0    0.0                         0.0       0.0   \n",
       "4           0.0            0.0    0.0                         0.0       0.0   \n",
       "\n",
       "   Lung Lesion  Lung Opacity  No Finding  Pleural Effusion  Pleural Other  \\\n",
       "0          0.0           0.0         1.0               0.0            0.0   \n",
       "1          0.0           0.0         1.0               0.0            0.0   \n",
       "2          0.0           0.0         0.0               0.0            0.0   \n",
       "3          0.0           0.0         0.0               0.0            0.0   \n",
       "4          0.0           1.0         0.0               0.0            0.0   \n",
       "\n",
       "   Pneumonia  Pneumothorax  Support Devices  \n",
       "0        0.0           0.0              1.0  \n",
       "1        0.0           0.0              1.0  \n",
       "2        0.0           0.0              0.0  \n",
       "3        0.0           0.0              0.0  \n",
       "4        1.0           0.0              1.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 44.9 ms\n"
     ]
    }
   ],
   "source": [
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 933 ms\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optin\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as tfunc\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "from tqdm import tqdm, trange\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "use_gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 168 ms\n"
     ]
    }
   ],
   "source": [
    "sample_train_data = train_data[0:10000]\n",
    "sample_train_data.to_csv('../data/sample_train_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.18 ms\n"
     ]
    }
   ],
   "source": [
    "pathFileSampleTrain = '../data/sample_train_data.csv'\n",
    "pathFileTrain = '../data/train_data.csv'\n",
    "pathFileValid = '../data/val_data.csv'\n",
    "\n",
    "nnIsTrained = False\n",
    "nnClassCount = 14\n",
    "\n",
    "trBatchSize = 64\n",
    "trMaxEpoch = 3\n",
    "\n",
    "imgtransResize = (320, 320)\n",
    "imgtransCrop = 224\n",
    "\n",
    "class_names = ['No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity', \n",
    "               'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis', 'Pneumothorax', \n",
    "               'Pleural Effusion', 'Pleural Other', 'Fracture', 'Support Devices']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.48 ms\n"
     ]
    }
   ],
   "source": [
    "class CheXpertDataSet(Dataset):\n",
    "    def __init__(self, image_list_file, transform=None, policy=\"ones\"):\n",
    "        \"\"\"\n",
    "        image_list_file: path to the file containing images with corresponding labels.\n",
    "        transform: optional transform to be applied on a sample.\n",
    "        policy: name the policy with regard to the uncertain labels\n",
    "        \"\"\"\n",
    "        image_names = []\n",
    "        labels = []\n",
    "\n",
    "        with open(image_list_file, \"r\") as f:\n",
    "            csvReader = csv.reader(f)\n",
    "            next(csvReader, None)\n",
    "            k=0\n",
    "            for line in csvReader:\n",
    "                k+=1\n",
    "                image_name= line[1]\n",
    "                label = line[5:]\n",
    "                \n",
    "                for i in range(14):\n",
    "                    label[i] = float(label[i])\n",
    "                \n",
    "                image_names.append(image_name)\n",
    "                labels.append(label)\n",
    "\n",
    "        self.image_names = image_names\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Take the index of item and returns the image and its labels\"\"\"\n",
    "        \n",
    "        image_name = self.image_names[index]\n",
    "        image = Image.open(image_name).convert('RGB')\n",
    "        label = self.labels[index]\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return (image, torch.FloatTensor(label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.image_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.18 ms\n"
     ]
    }
   ],
   "source": [
    "#TRANSFORM DATA\n",
    "\n",
    "normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "transformList = []\n",
    "transformList.append(transforms.RandomResizedCrop(imgtransCrop))\n",
    "transformList.append(transforms.RandomHorizontalFlip())\n",
    "transformList.append(transforms.ToTensor())\n",
    "transformList.append(normalize)      \n",
    "transformSequence=transforms.Compose(transformList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Simple Train Dataset - 10000\n",
      "Size of Train Dataset - 368945\n",
      "time: 2.27 s\n"
     ]
    }
   ],
   "source": [
    "#LOAD DATASET\n",
    "\n",
    "datasetSampleTrain = CheXpertDataSet(pathFileSampleTrain, transformSequence, policy = 'ones')\n",
    "datasetTrain = CheXpertDataSet(pathFileTrain ,transformSequence, policy = 'ones')\n",
    "datasetValid = CheXpertDataSet(pathFileValid, transformSequence)\n",
    "\n",
    "print ('Size of Simple Train Dataset - {}'.format(len(datasetSampleTrain)))\n",
    "print ('Size of Train Dataset - {}'.format(len(datasetTrain)))\n",
    "\n",
    "dataLoaderSampleTrain = DataLoader(dataset=datasetSampleTrain, batch_size=trBatchSize, shuffle=True, num_workers=0, pin_memory = True)\n",
    "dataLoaderTrain = DataLoader(dataset=datasetTrain, batch_size=trBatchSize, shuffle=True,  num_workers=0, pin_memory = True)\n",
    "dataLoaderVal = DataLoader(dataset=datasetValid, batch_size=trBatchSize, shuffle=False, num_workers=0, pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7.19 ms\n"
     ]
    }
   ],
   "source": [
    "class CheXpertTrainer():\n",
    "\n",
    "    def train (model, dataLoaderTrain, dataLoaderVal, nnClassCount, trMaxEpoch, launchTimestamp, checkpoint):\n",
    "        \n",
    "        #SETTINGS: OPTIMIZER & SCHEDULER\n",
    "        optimizer = optim.Adam (model.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-5)\n",
    "                \n",
    "        #SETTINGS: LOSS\n",
    "        loss = torch.nn.BCELoss(size_average = True)\n",
    "        \n",
    "        #LOAD CHECKPOINT \n",
    "        if checkpoint != None and use_gpu:\n",
    "            modelCheckpoint = torch.load(checkpoint)\n",
    "            model.load_state_dict(modelCheckpoint['state_dict'])\n",
    "            optimizer.load_state_dict(modelCheckpoint['optimizer'])\n",
    "da\n",
    "        \n",
    "        #TRAIN THE NETWORK\n",
    "        lossMIN = 100000\n",
    "        \n",
    "        for epochID in range(0, trMaxEpoch):\n",
    "            \n",
    "            timestampTime = time.strftime(\"%H%M%S\")\n",
    "            timestampDate = time.strftime(\"%d%m%Y\")\n",
    "            timestampSTART = timestampDate + '-' + timestampTime\n",
    "            \n",
    "            batchs, losst, losse = CheXpertTrainer.epochTrain(model, dataLoaderTrain, optimizer, trMaxEpoch, nnClassCount, loss)\n",
    "            lossVal = CheXpertTrainer.epochVal(model, dataLoaderVal, optimizer, trMaxEpoch, nnClassCount, loss)\n",
    "\n",
    "\n",
    "            timestampTime = time.strftime(\"%H%M%S\")\n",
    "            timestampDate = time.strftime(\"%d%m%Y\")\n",
    "            timestampEND = timestampDate + '-' + timestampTime\n",
    "            \n",
    "            if lossVal < lossMIN:\n",
    "                lossMIN = lossVal    \n",
    "                torch.save({'epoch': epochID + 1, 'state_dict': model.state_dict(), 'best_loss': lossMIN, 'optimizer' : optimizer.state_dict()}, 'm-epoch'+str(epochID)+'-' + launchTimestamp + '.pth.tar')\n",
    "                print ('Epoch [' + str(epochID + 1) + '] [save] [' + timestampEND + '] loss= ' + str(lossVal))\n",
    "            else:\n",
    "                print ('Epoch [' + str(epochID + 1) + '] [----] [' + timestampEND + '] loss= ' + str(lossVal))\n",
    "        \n",
    "        return (batchs, losst, losse)\n",
    "    #-------------------------------------------------------------------------------- \n",
    "       \n",
    "    def epochTrain(model, dataLoader, optimizer, epochMax, classCount, loss):\n",
    "        \n",
    "        batch = []\n",
    "        losstrain = []\n",
    "        losseval = []\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        for batchID, (varInput, target) in enumerate(dataLoaderTrain):\n",
    "            \n",
    "            varTarget = target.cuda(non_blocking = True)\n",
    "            \n",
    "            #varTarget = target.cuda()         \n",
    "            \n",
    "            print (varInput.shape)\n",
    "            varOutput = model(varInput)\n",
    "            lossvalue = loss(varOutput, varTarget)\n",
    "                       \n",
    "            optimizer.zero_grad()\n",
    "            lossvalue.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            l = lossvalue.item()\n",
    "            losstrain.append(l)\n",
    "            \n",
    "            if batchID%35==0:\n",
    "                logger.info('Batches Computed - {}'.format(batchID//35))\n",
    "                print(batchID//35, \"% batches computed\")\n",
    "                #Fill three arrays to see the evolution of the loss\n",
    "\n",
    "\n",
    "                batch.append(batchID)\n",
    "                \n",
    "                le = CheXpertTrainer.epochVal(model, dataLoaderVal, optimizer, trMaxEpoch, nnClassCount, loss).item()\n",
    "                losseval.append(le)\n",
    "                \n",
    "                logger.info('Batch ID - {}'.format(batchID))\n",
    "                logger.info('Training Loss - {}'.format(l))\n",
    "                logger.info('Validation Loss - {}'.format(le))\n",
    "                \n",
    "                print('Batch ID - {}'.format(batchID))\n",
    "                print('Training Loss - {}'.format(l))\n",
    "                print('Validation Loss - {}'.format(le))\n",
    "                \n",
    "        return (batch, losstrain, losseval)\n",
    "    \n",
    "    #-------------------------------------------------------------------------------- \n",
    "    \n",
    "    def epochVal(model, dataLoader, optimizer, epochMax, classCount, loss):\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        lossVal = 0\n",
    "        lossValNorm = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, (varInput, target) in enumerate(dataLoaderVal):\n",
    "                \n",
    "                target = target.cuda(non_blocking = True)\n",
    "                varOutput = model(varInput)\n",
    "                \n",
    "                losstensor = loss(varOutput, target)\n",
    "                lossVal += losstensor\n",
    "                lossValNorm += 1\n",
    "                \n",
    "        outLoss = lossVal / lossValNorm\n",
    "        return (outLoss)\n",
    "    \n",
    "    \n",
    "    #--------------------------------------------------------------------------------     \n",
    "     \n",
    "    #---- Computes area under ROC curve \n",
    "    #---- dataGT - ground truth data\n",
    "    #---- dataPRED - predicted data\n",
    "    #---- classCount - number of classes\n",
    "    \n",
    "    def computeAUROC (dataGT, dataPRED, classCount):\n",
    "        \n",
    "        outAUROC = []\n",
    "        \n",
    "        datanpGT = dataGT.cpu().numpy()\n",
    "        datanpPRED = dataPRED.cpu().numpy()\n",
    "        \n",
    "        for i in range(classCount):\n",
    "            try:\n",
    "                outAUROC.append(roc_auc_score(datanpGT[:, i], datanpPRED[:, i]))\n",
    "            except ValueError:\n",
    "                pass\n",
    "        return (outAUROC)\n",
    "        \n",
    "        \n",
    "    #-------------------------------------------------------------------------------- \n",
    "    \n",
    "    \n",
    "    def test(model, dataLoaderTest, nnClassCount, checkpoint, class_names):   \n",
    "        \n",
    "        cudnn.benchmark = True\n",
    "        \n",
    "        if checkpoint != None and use_gpu:\n",
    "            modelCheckpoint = torch.load(checkpoint)\n",
    "            model.load_state_dict(modelCheckpoint['state_dict'])\n",
    "\n",
    "        if use_gpu:\n",
    "            outGT = torch.FloatTensor().cuda()\n",
    "            outPRED = torch.FloatTensor().cuda()\n",
    "        else:\n",
    "            outGT = torch.FloatTensor()\n",
    "            outPRED = torch.FloatTensor()\n",
    "       \n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, (input, target) in enumerate(dataLoaderTest):\n",
    "\n",
    "                target = target.cuda()\n",
    "                outGT = torch.cat((outGT, target), 0).cuda()\n",
    "                \n",
    "                bs, c, h, w = input.size()\n",
    "                varInput = input.view(-1, c, h, w)\n",
    "            \n",
    "                out = model(varInput)\n",
    "                outPRED = torch.cat((outPRED, out), 0)\n",
    "        aurocIndividual = CheXpertTrainer.computeAUROC(outGT, outPRED, nnClassCount)\n",
    "        aurocMean = np.array(aurocIndividual).mean()\n",
    "        \n",
    "        logger.info('AUROC mean ', aurocMean)\n",
    "        print ('AUROC mean ', aurocMean)\n",
    "        \n",
    "        for i in range (0, len(aurocIndividual)):\n",
    "            print (class_names[i], ' ', aurocIndividual[i])\n",
    "        \n",
    "        return (outGT, outPRED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 895 µs\n"
     ]
    }
   ],
   "source": [
    "class DenseNet121(nn.Module):\n",
    "    \"\"\"Model modified.\n",
    "    The architecture of our model is the same as standard DenseNet121\n",
    "    except the classifier layer which has an additional sigmoid function.\n",
    "    \"\"\"\n",
    "    def __init__(self, out_size):\n",
    "        super(DenseNet121, self).__init__()\n",
    "        self.densenet121 = torchvision.models.densenet121(pretrained=True)\n",
    "        num_ftrs = self.densenet121.classifier.in_features\n",
    "        self.densenet121.classifier = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, out_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.densenet121(x)\n",
    "        return (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.5 s\n"
     ]
    }
   ],
   "source": [
    "# initialize and load the model\n",
    "\n",
    "model = DenseNet121(nnClassCount).cuda()\n",
    "model = torch.nn.DataParallel(model).cuda()\n",
    "logger.info('Initial model is loaded on to GPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 159 ms\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = torch.load('../model/model_ones_3epoch_densenet.tar')\n",
    "model_state_dict = pretrained_model['state_dict']\n",
    "torch.save(model_state_dict, '../model/model_state_dict.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): DenseNet121(\n",
       "    (densenet121): DenseNet(\n",
       "      (features): Sequential(\n",
       "        (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "        (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu0): ReLU(inplace=True)\n",
       "        (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "        (denseblock1): _DenseBlock(\n",
       "          (denselayer1): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer2): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer3): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer4): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer5): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer6): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (transition1): _Transition(\n",
       "          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        )\n",
       "        (denseblock2): _DenseBlock(\n",
       "          (denselayer1): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer2): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer3): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer4): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer5): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer6): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer7): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer8): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer9): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer10): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer11): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer12): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (transition2): _Transition(\n",
       "          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        )\n",
       "        (denseblock3): _DenseBlock(\n",
       "          (denselayer1): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer2): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer3): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer4): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer5): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer6): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer7): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer8): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer9): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer10): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer11): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer12): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer13): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer14): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer15): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer16): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer17): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer18): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer19): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer20): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer21): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer22): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer23): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer24): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (transition3): _Transition(\n",
       "          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        )\n",
       "        (denseblock4): _DenseBlock(\n",
       "          (denselayer1): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer2): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer3): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer4): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer5): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer6): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer7): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer8): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer9): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer10): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer11): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer12): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer13): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer14): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer15): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer16): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (classifier): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=14, bias=True)\n",
       "        (1): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 100 ms\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('../model/model_state_dict.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC mean  0.527075225168402\n",
      "No Finding   0.2608430020766045\n",
      "Enlarged Cardiomediastinum   0.48847670167915547\n",
      "Cardiomegaly   0.5818568200043741\n",
      "Lung Opacity   0.7869942444553917\n",
      "Lung Lesion   0.4057614511961206\n",
      "Edema   0.49344914364772674\n",
      "Consolidation   0.5791579601316831\n",
      "Pneumonia   0.6919774343070435\n",
      "Atelectasis   0.22803036526434659\n",
      "Pneumothorax   0.4570719185780653\n",
      "Pleural Effusion   0.606292409468902\n",
      "Pleural Other   0.505209144749428\n",
      "Fracture   0.4341911618062391\n",
      "Support Devices   0.8597413949925465\n",
      "time: 9min 36s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/tljh/user/lib/python3.7/logging/__init__.py\", line 1034, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"/opt/tljh/user/lib/python3.7/logging/__init__.py\", line 880, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/opt/tljh/user/lib/python3.7/logging/__init__.py\", line 619, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"/opt/tljh/user/lib/python3.7/logging/__init__.py\", line 380, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"/opt/tljh/user/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/opt/tljh/user/lib/python3.7/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/traitlets/config/application.py\", line 664, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 583, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 132, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/tljh/user/lib/python3.7/asyncio/base_events.py\", line 539, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/tljh/user/lib/python3.7/asyncio/base_events.py\", line 1775, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/tljh/user/lib/python3.7/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/tornado/gen.py\", line 1233, in inner\n",
      "    self.run()\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/tornado/gen.py\", line 1147, in run\n",
      "    yielded = self.gen.send(value)\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 381, in dispatch_queue\n",
      "    yield self.process_one()\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/tornado/gen.py\", line 346, in wrapper\n",
      "    runner = Runner(result, future, yielded)\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/tornado/gen.py\", line 1080, in __init__\n",
      "    self.run()\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/tornado/gen.py\", line 1147, in run\n",
      "    yielded = self.gen.send(value)\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 365, in process_one\n",
      "    yield gen.maybe_future(dispatch(*args))\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/tornado/gen.py\", line 326, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n",
      "    yield gen.maybe_future(handler(stream, idents, msg))\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/tornado/gen.py\", line 326, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 545, in execute_request\n",
      "    user_expressions, allow_stdin,\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/tornado/gen.py\", line 326, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 300, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2858, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2886, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3063, in run_cell_async\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3254, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-17-363c538dcbf7>\", line 1, in <module>\n",
      "    outGT, outPRED = CheXpertTrainer.test(model = model, dataLoaderTest = dataLoaderSampleTrain, nnClassCount = nnClassCount, class_names = class_names, checkpoint = None)\n",
      "  File \"<ipython-input-12-ff2ad4d240f5>\", line 170, in test\n",
      "    logger.info('AUROC mean ', aurocMean)\n",
      "Message: 'AUROC mean '\n",
      "Arguments: (0.527075225168402,)\n"
     ]
    }
   ],
   "source": [
    "outGT, outPRED = CheXpertTrainer.test(model = model, dataLoaderTest = dataLoaderSampleTrain, nnClassCount = nnClassCount, class_names = class_names, checkpoint = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 18 ms\n"
     ]
    }
   ],
   "source": [
    "outPRED = outPRED.cpu().detach().numpy()\n",
    "outPRED = np.array(outPRED)\n",
    "\n",
    "sample_train_data['pred_No Finding'] = np.round(outPRED[:,0])\n",
    "sample_train_data['pred_Enlarged Cardiomediastinum'] = np.round(outPRED[:,1])\n",
    "sample_train_data['pred_Cardiomegaly'] = np.round(outPRED[:,2])\n",
    "sample_train_data['pred_Lung Opacity'] = np.round(outPRED[:,3])\n",
    "sample_train_data['pred_Lung Lesion'] = np.round(outPRED[:,4])\n",
    "sample_train_data['pred_Edema'] = np.round(outPRED[:,5])\n",
    "sample_train_data['pred_Consolidation'] = np.round(outPRED[:,6])\n",
    "sample_train_data['pred_Pneumonia'] = np.round(outPRED[:,7])\n",
    "sample_train_data['pred_Atelectasis'] = np.round(outPRED[:,8])\n",
    "sample_train_data['pred_Pneumothorax'] = np.round(outPRED[:,9])\n",
    "sample_train_data['pred_Pleural Effusion'] = np.round(outPRED[:,10])\n",
    "sample_train_data['pred_Pleural Other'] = np.round(outPRED[:,11])\n",
    "sample_train_data['pred_Fracture'] = np.round(outPRED[:,12])\n",
    "sample_train_data['pred_Support Devices'] = np.round(outPRED[:,13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.18 ms\n"
     ]
    }
   ],
   "source": [
    "len(sample_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.23 ms\n"
     ]
    }
   ],
   "source": [
    "'pred_No Finding' in sample_train_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo List\n",
    "\n",
    "- Train few shot learning model on every class with 100 images.\n",
    "    - Incrementally build the few shot learning model on all the 14 conditions.\n",
    "- Validate the results on the all images (except the 10 images).\n",
    "- Compare the results with the prior results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 753 µs\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    'train_size' : 100,\n",
    "    'test_size' : 100,\n",
    "    'out_size' : 128, # should be less than 512\n",
    "    'loss_margin' : 0.2,\n",
    "    'loss_p' : 2,\n",
    "    'batch_size' : 4 # do not change this\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 24.8 ms\n"
     ]
    }
   ],
   "source": [
    "class DenseNet121ToDense(nn.Module):\n",
    "    '''\n",
    "    The architecture of the densenet121 is copied and the final classifier layer is removed.\n",
    "    In the place of final classifier layer a dense layer is attached with PReLU activation.\n",
    "    '''\n",
    "    def __init__(self, out_size):\n",
    "        super(DenseNet121ToDense, self).__init__()\n",
    "        self.densenet121todense = model.module.densenet121\n",
    "        num_ftrs = self.densenet121todense.classifier[0].in_features\n",
    "        self.densenet121todense.classifier = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, 512),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(512, out_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_1, x_2, x_3):\n",
    "        x_1 = self.densenet121todense(x_1)\n",
    "        x_2 = self.densenet121todense(x_2)\n",
    "        x_3 = self.densenet121todense(x_3)\n",
    "        return (x_1, x_2, x_3)\n",
    "    \n",
    "ref_model = DenseNet121ToDense(args['out_size']).cuda()\n",
    "ref_model = torch.nn.DataParallel(ref_model).cuda()\n",
    "logger.info('FSL model is loaded on to GPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.04 ms\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, TensorDataset\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def make_train_triplets(pathology, sample_train_data, n):\n",
    "    '''\n",
    "    Returns 'n' triplet images for training data for a given pathology.\n",
    "    First Image - Any random image (with atleast 40 images which has the pathology)\n",
    "    Second Image - An image which is positive for the pathology.\n",
    "    Third Image - An image which is negative for the pathology.\n",
    "    '''\n",
    "    \n",
    "    actual_path = pathology\n",
    "    pred_path = 'pred_'+pathology\n",
    "    false_negative = sample_train_data[(sample_train_data[actual_path] == 1.0) & (sample_train_data[pred_path] == 0.0)]\n",
    "    false_positive = sample_train_data[(sample_train_data[actual_path] == 0.0) & (sample_train_data[pred_path] == 1.0)]\n",
    "    print ('False Negatives - {} False Positives - {}'.format(len(false_negative), len(false_positive)))\n",
    "\n",
    "    positive = sample_train_data[sample_train_data[actual_path] == 1.0]\n",
    "    negative = sample_train_data[sample_train_data[actual_path] == 0.0]\n",
    "    \n",
    "    images = []\n",
    "    checking_label = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        if (len(false_negative.index) > 0 and len(false_positive.index) > 0):\n",
    "            choose = random.choice(['a','b'])    \n",
    "            if (choose == 'a'):\n",
    "                first_image_index = random.choice(false_negative.index)\n",
    "                checking_label.append(-1)\n",
    "            elif (choose == 'b'):\n",
    "                first_image_index = random.choice(false_positive.index)\n",
    "                checking_label.append(1)\n",
    "        else:\n",
    "            if (len(false_negative.index) > 0):\n",
    "                first_image_index = random.choice(false_negative.index)\n",
    "                checking_label.append(-1)\n",
    "            elif (len(false_positive.index) > 0):\n",
    "                first_image_index = random.choice(false_positive.index)\n",
    "                checking_label.append(1)\n",
    "            \n",
    "        second_image_index = random.choice(positive.index)\n",
    "        third_image_index = random.choice(negative.index)\n",
    "\n",
    "        # Convert the indices to images\n",
    "        first_image = sample_train_data['image_path'][first_image_index]\n",
    "        first_image = Image.open(first_image).convert('RGB')\n",
    "        first_image = transformSequence(first_image)\n",
    "        \n",
    "        second_image = sample_train_data['image_path'][second_image_index]\n",
    "        second_image = Image.open(second_image).convert('RGB')\n",
    "        second_image = transformSequence(second_image)\n",
    "        \n",
    "        third_image = sample_train_data['image_path'][third_image_index]\n",
    "        third_image = Image.open(third_image).convert('RGB')\n",
    "        third_image = transformSequence(third_image)\n",
    "        \n",
    "        images += [[first_image, second_image, third_image]]\n",
    "        \n",
    "    return (images, checking_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.56 ms\n"
     ]
    }
   ],
   "source": [
    "def make_test_triplets(pathology, sample_train_data):\n",
    "    '''\n",
    "    - Returns three images (so the name triplets)\n",
    "    - First Image : An Inference failed image. (Inference failures can appear in the form of false negatives and \n",
    "    false postives)\n",
    "    - Second Image: An image which is positive for the pathology.\n",
    "    - Third Image: An image which is negative for the pathology.\n",
    "    '''\n",
    "    \n",
    "    actual_path = pathology\n",
    "    pred_path = 'pred_'+pathology\n",
    "    false_negative = sample_train_data[(sample_train_data[actual_path] == 1.0) & (sample_train_data[pred_path] == 0.0)]\n",
    "    false_positive = sample_train_data[(sample_train_data[actual_path] == 0.0) & (sample_train_data[pred_path] == 1.0)]\n",
    "    print ('False Negatives - {} False Positives - {}'.format(len(false_negative), len(false_positive)))\n",
    "\n",
    "    positive = sample_train_data[sample_train_data[actual_path] == 1.0]\n",
    "    negative = sample_train_data[sample_train_data[actual_path] == 0.0]\n",
    "    \n",
    "    images = []\n",
    "    checking_label = []\n",
    "    \n",
    "    false_inference_index = false_negative.index.union(false_positive.index)\n",
    "    for index in false_inference_index:\n",
    "        if index in false_negative.index:\n",
    "            checking_label.append(-1)\n",
    "        elif index in false_positive.index:\n",
    "            checking_label.append(1)\n",
    "            \n",
    "        first_image_index  = index    \n",
    "        second_image_index = random.choice(positive.index)\n",
    "        third_image_index = random.choice(negative.index)\n",
    "\n",
    "        # Convert the indices to images\n",
    "        first_image = sample_train_data['image_path'][first_image_index]\n",
    "        first_image = Image.open(first_image).convert('RGB')\n",
    "        first_image = transformSequence(first_image)\n",
    "\n",
    "        second_image = sample_train_data['image_path'][second_image_index]\n",
    "        second_image = Image.open(second_image).convert('RGB')\n",
    "        second_image = transformSequence(second_image)\n",
    "\n",
    "        third_image = sample_train_data['image_path'][third_image_index]\n",
    "        third_image = Image.open(third_image).convert('RGB')\n",
    "        third_image = transformSequence(third_image)\n",
    "\n",
    "        images += [[first_image, second_image, third_image]]\n",
    "        \n",
    "    return (images, checking_label, false_inference_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.51 ms\n"
     ]
    }
   ],
   "source": [
    "def ref_train_image_triplets(images, checking_label):\n",
    "    first_images = [i[0] for i in images]\n",
    "    second_images = [i[1] for i in images]\n",
    "    third_images = [i[2] for i in images]\n",
    "\n",
    "    first_images = torch.stack(first_images)\n",
    "    second_images = torch.stack(second_images)\n",
    "    third_images = torch.stack(third_images)\n",
    "    \n",
    "    target = torch.Tensor(checking_label).int()    \n",
    "    return (first_images, second_images, third_images, target)\n",
    "\n",
    "\n",
    "def ref_test_image_triplets(test_images, test_checking_label):\n",
    "    test_first_images = [i[0] for i in test_images]\n",
    "    test_second_images = [i[1] for i in test_images]\n",
    "    test_third_images = [i[2] for i in test_images]\n",
    "    \n",
    "    test_first_images = torch.stack(test_first_images)\n",
    "    test_second_images = torch.stack(test_second_images)\n",
    "    test_third_images = torch.stack(test_third_images)\n",
    "    \n",
    "    test_target = torch.Tensor(test_checking_label).int()\n",
    "    return (test_first_images, test_second_images, test_third_images, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7.01 ms\n"
     ]
    }
   ],
   "source": [
    "def triplet_distance(anchor, positive, negative):\n",
    "    '''\n",
    "    The function calculates the distance between anchor, positive and anchor, negative images. The difference \n",
    "    between the pos_distance, neg_distance was calculated and tuned according to the checking_label.\n",
    "    '''\n",
    "    pos_distance = F.pairwise_distance(anchor, positive, 2)\n",
    "    neg_distance = F.pairwise_distance(anchor, negative, 2)\n",
    "    return (pos_distance, neg_distance)\n",
    "\n",
    "def predictive_value(conf_matrix):\n",
    "    '''\n",
    "    This function returns the positive predictive value and negative predictive value of a classifier.\n",
    "    Parameters:\n",
    "    \n",
    "    1. conf_matrix : A Confusion matrix from the result of a classifier.\n",
    "    '''\n",
    "    ppv = (conf_matrix[0][0]/(conf_matrix[0][0] + conf_matrix[0][1]))*100\n",
    "    npv = (conf_matrix[1][1]/(conf_matrix[1][0] + conf_matrix[1][1]))*100\n",
    "    return (ppv, npv)\n",
    "\n",
    "def train_and_eval(ref_model, train_dataloader, test_dataloader):\n",
    "    for epoch in trange(5):\n",
    "    \n",
    "        tr_loss = 0\n",
    "        nb_tr_steps = 0\n",
    "        ref_model.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            anchor, positive, negative, target = batch[0], batch[1], batch[2], batch[3]\n",
    "            anchor, positive, negative, target = Variable(anchor), Variable(positive), Variable(negative), Variable(target)\n",
    "\n",
    "            bs, c, h, w = anchor.size()\n",
    "            anchor_input = anchor.view(-1, c, h, w)\n",
    "            bs, c, h, w = positive.size()\n",
    "            positive_input = positive.view(-1, c, h, w)\n",
    "            bs, c, h, w = negative.size()\n",
    "            negative_input = negative.view(-1, c, h, w)\n",
    "\n",
    "            E1, E2, E3 = ref_model(anchor_input, positive_input, negative_input)\n",
    "            dist_E1_E2, dist_E1_E3 = triplet_distance(E1, E2, E3)\n",
    "\n",
    "            target = target.cuda()\n",
    "            loss = criterion(dist_E1_E2, dist_E1_E3, target)\n",
    "            tr_loss += loss\n",
    "            nb_tr_steps += 1\n",
    "\n",
    "            torch.autograd.set_detect_anomaly(True)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph = True)\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "\n",
    "        pred_list = []\n",
    "        with torch.no_grad():\n",
    "            ref_model.eval()\n",
    "            for step, batch in enumerate(test_dataloader):\n",
    "                anchor, positive, negative, target = batch[0], batch[1], batch[2], batch[3]\n",
    "                anchor, positive, negative, target = Variable(anchor), Variable(positive), Variable(negative), Variable(target)\n",
    "\n",
    "                bs, c, h, w = anchor.size()\n",
    "                anchor_input = anchor.view(-1, c, h, w)\n",
    "                bs, c, h, w = positive.size()\n",
    "                positive_input = positive.view(-1, c, h, w)\n",
    "                bs, c, h, w = negative.size()\n",
    "                negative_input = negative.view(-1, c, h, w)\n",
    "\n",
    "                E1, E2, E3 = ref_model(anchor_input, positive_input, negative_input)\n",
    "                dist_E1_E2, dist_E1_E3 = triplet_distance(E1, E2, E3)\n",
    "\n",
    "                for i in range(len(dist_E1_E2)):\n",
    "                    if (dist_E1_E2[i] > dist_E1_E3[i]):\n",
    "                        pred_list.append(1)\n",
    "                    else:\n",
    "                        pred_list.append(0)\n",
    "\n",
    "        mod_test_target = []\n",
    "        for i in test_target:\n",
    "            if i == -1:\n",
    "                mod_test_target.append(1)\n",
    "            else:\n",
    "                mod_test_target.append(0)\n",
    "    \n",
    "    return (mod_test_target, pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Name : No Finding\n",
      "False Negatives - 3785 False Positives - 578\n",
      "False Negatives - 3785 False Positives - 578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1993531882762909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 1/5 [06:31<26:07, 391.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.09878809005022049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [12:47<19:20, 386.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.024048661813139915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [19:00<12:45, 382.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.008653320372104645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [25:22<06:22, 382.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0029197020921856165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [31:44<00:00, 382.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before FSL - \n",
      "Positive Predictive Value : 89.99653859466945\n",
      "Negative Predictive Value : 10.350544765513975\n",
      "After FSL - \n",
      "Positive Predictive Value : 94.06368985808238\n",
      "Negative Predictive Value : 44.38654666035055\n",
      "--------------------------------------\n",
      "Class Name : Enlarged Cardiomediastinum\n",
      "False Negatives - 565 False Positives - 0\n",
      "False Negatives - 565 False Positives - 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.19160529971122742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 1/5 [02:12<08:50, 132.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.029144054278731346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [04:28<06:40, 133.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.005912546068429947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [06:45<04:29, 134.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.00091033976059407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [09:01<02:15, 135.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0003031252126675099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [11:19<00:00, 135.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before FSL - \n",
      "Positive Predictive Value : 100.0\n",
      "Negative Predictive Value : 0.0\n",
      "After FSL - \n",
      "Positive Predictive Value : 100.0\n",
      "Negative Predictive Value : 40.0\n",
      "--------------------------------------\n",
      "Class Name : Cardiomegaly\n",
      "False Negatives - 1671 False Positives - 922\n",
      "False Negatives - 1671 False Positives - 922\n"
     ]
    }
   ],
   "source": [
    "for _class in class_names:\n",
    "    \n",
    "    actual_class = _class\n",
    "    pred_class = 'pred_'+_class\n",
    "    print ('Class Name : {}'.format(actual_class))\n",
    "    logger.info('Class Name : {}'.format(actual_class))\n",
    "    images, checking_label = make_train_triplets(_class, sample_train_data, 150)\n",
    "    test_images, test_checking_label, false_inference_index = make_test_triplets(_class, sample_train_data)\n",
    "    \n",
    "    first_images, second_images, third_images, target = ref_train_image_triplets(images, checking_label)\n",
    "    test_first_images, test_second_images, test_third_images, test_target = ref_test_image_triplets(test_images, test_checking_label)\n",
    "    \n",
    "    ref_train_data = TensorDataset(first_images, second_images, third_images, target)\n",
    "    train_dataloader = DataLoader(ref_train_data, batch_size = args['batch_size'], shuffle = True, num_workers = 8, pin_memory = True)\n",
    "\n",
    "    test_data = TensorDataset(test_first_images, test_second_images, test_third_images, test_target)\n",
    "    test_dataloader = DataLoader(test_data, batch_size = args['batch_size'], shuffle = False, num_workers = 8, pin_memory = True)\n",
    "\n",
    "    criterion = torch.nn.MarginRankingLoss(margin = args['loss_margin'])\n",
    "    optimizer = optim.Adam (model.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-5)\n",
    "    \n",
    "    mod_test_target, pred_list = train_and_eval(ref_model, train_dataloader, test_dataloader)\n",
    "    \n",
    "    # Calculating the ROC-AUC Scores before and after the few shot learning algorithm.\n",
    "    sample_train_data_copy_pred_class = copy.deepcopy(sample_train_data[pred_class])\n",
    "    i = 0\n",
    "    for index in false_inference_index:\n",
    "        sample_train_data_copy_pred_class[index] = pred_list[i]\n",
    "        i += 1\n",
    "        \n",
    "    sample_train_data['FSL_pred_'+_class] = sample_train_data_copy_pred_class\n",
    "    \n",
    "    print ('Before FSL - ') \n",
    "    logger.info('Before FSL - ')\n",
    "    ppv, npv = predictive_value(confusion_matrix(sample_train_data[actual_class], sample_train_data[pred_class]))\n",
    "    print ('Positive Predictive Value : {}'.format(ppv))\n",
    "    logger.info('Positive Predictive Value : {}'.format(ppv))\n",
    "    print ('Negative Predictive Value : {}'.format(npv))\n",
    "    logger.info('Negative Predictive Value : {}'.format(npv))\n",
    "    \n",
    "    print ('After FSL - ')\n",
    "    logger.info('After FSL - ')\n",
    "    ppv, npv = predictive_value(confusion_matrix(sample_train_data[actual_class], sample_train_data_copy_pred_class))\n",
    "    print ('Positive Predictive Value : {}'.format(ppv))\n",
    "    logger.info('Positive Predictive Value : {}'.format(ppv))\n",
    "    print ('Negative Predictive Value : {}'.format(npv))\n",
    "    logger.info('Negative Predictive Value : {}'.format(npv))\n",
    "    print ('--------------------------------------')\n",
    "    logger.info('--------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
