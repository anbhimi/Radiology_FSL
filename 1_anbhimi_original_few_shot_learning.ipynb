{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autotime\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "random.seed(1)\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 320 µs\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4, 5, 6, 7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 701 µs\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "log_format = '%(levelname)s %(asctime)s - %(message)s'\n",
    "logging.basicConfig(filename = '../logs/original_fsl.logs',\n",
    "                    level = logging.INFO,\n",
    "                    format = log_format,\n",
    "                    filemode = 'w')\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(368945, 19)\n",
      "Index(['subject_id', 'image_path', 'image_name', 'study_id', 'split',\n",
      "       'Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema',\n",
      "       'Enlarged Cardiomediastinum', 'Fracture', 'Lung Lesion', 'Lung Opacity',\n",
      "       'No Finding', 'Pleural Effusion', 'Pleural Other', 'Pneumonia',\n",
      "       'Pneumothorax', 'Support Devices'],\n",
      "      dtype='object')\n",
      "time: 928 ms\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('../data/train_data.csv')\n",
    "print (train_data.shape)\n",
    "print (train_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>image_path</th>\n",
       "      <th>image_name</th>\n",
       "      <th>study_id</th>\n",
       "      <th>split</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Enlarged Cardiomediastinum</th>\n",
       "      <th>Fracture</th>\n",
       "      <th>Lung Lesion</th>\n",
       "      <th>Lung Opacity</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Pleural Effusion</th>\n",
       "      <th>Pleural Other</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Support Devices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s52769454</td>\n",
       "      <td>../../DataCenter/MIMIC-CXR/files/p18/p18190098...</td>\n",
       "      <td>8bf006fa-7169cd83-c30e7055-b109468a-2223477d</td>\n",
       "      <td>52769454</td>\n",
       "      <td>train</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s50754262</td>\n",
       "      <td>../../DataCenter/MIMIC-CXR/files/p18/p18190098...</td>\n",
       "      <td>48fbe534-50750d68-5afd36c2-e7aaf316-a31fcb66</td>\n",
       "      <td>50754262</td>\n",
       "      <td>train</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s51845898</td>\n",
       "      <td>../../DataCenter/MIMIC-CXR/files/p18/p18190098...</td>\n",
       "      <td>fc2e907b-c19b5434-cc3d4205-8d66ab01-a0dcc274</td>\n",
       "      <td>51845898</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subject_id                                         image_path  \\\n",
       "0  s52769454  ../../DataCenter/MIMIC-CXR/files/p18/p18190098...   \n",
       "1  s50754262  ../../DataCenter/MIMIC-CXR/files/p18/p18190098...   \n",
       "2  s51845898  ../../DataCenter/MIMIC-CXR/files/p18/p18190098...   \n",
       "\n",
       "                                     image_name  study_id  split  Atelectasis  \\\n",
       "0  8bf006fa-7169cd83-c30e7055-b109468a-2223477d  52769454  train          0.0   \n",
       "1  48fbe534-50750d68-5afd36c2-e7aaf316-a31fcb66  50754262  train          0.0   \n",
       "2  fc2e907b-c19b5434-cc3d4205-8d66ab01-a0dcc274  51845898  train          1.0   \n",
       "\n",
       "   Cardiomegaly  Consolidation  Edema  Enlarged Cardiomediastinum  Fracture  \\\n",
       "0           0.0            0.0    0.0                         0.0       0.0   \n",
       "1           0.0            0.0    0.0                         0.0       0.0   \n",
       "2           0.0            0.0    0.0                         0.0       0.0   \n",
       "\n",
       "   Lung Lesion  Lung Opacity  No Finding  Pleural Effusion  Pleural Other  \\\n",
       "0          0.0           0.0         1.0               0.0            0.0   \n",
       "1          0.0           0.0         1.0               0.0            0.0   \n",
       "2          0.0           0.0         0.0               0.0            0.0   \n",
       "\n",
       "   Pneumonia  Pneumothorax  Support Devices  \n",
       "0        0.0           0.0              1.0  \n",
       "1        0.0           0.0              1.0  \n",
       "2        0.0           0.0              0.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 22.2 ms\n"
     ]
    }
   ],
   "source": [
    "train_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.25 s\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optin\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as tfunc\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "from tqdm import tqdm, trange\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "use_gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 231 ms\n"
     ]
    }
   ],
   "source": [
    "sample_train_data = train_data[0:10000]\n",
    "sample_train_data.to_csv('../data/sample_train_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.53 ms\n"
     ]
    }
   ],
   "source": [
    "pathFileSampleTrain = '../data/sample_train_data.csv'\n",
    "pathFileTrain = '../data/train_data.csv'\n",
    "pathFileValid = '../data/val_data.csv'\n",
    "\n",
    "nnIsTrained = False\n",
    "nnClassCount = 14\n",
    "\n",
    "trBatchSize = 64\n",
    "trMaxEpoch = 3\n",
    "\n",
    "imgtransResize = (320, 320)\n",
    "imgtransCrop = 224\n",
    "\n",
    "class_names = ['No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity', \n",
    "               'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis', 'Pneumothorax', \n",
    "               'Pleural Effusion', 'Pleural Other', 'Fracture', 'Support Devices']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.97 ms\n"
     ]
    }
   ],
   "source": [
    "class CheXpertDataSet(Dataset):\n",
    "    def __init__(self, image_list_file, transform=None, policy=\"ones\"):\n",
    "        \"\"\"\n",
    "        image_list_file: path to the file containing images with corresponding labels.\n",
    "        transform: optional transform to be applied on a sample.\n",
    "        policy: name the policy with regard to the uncertain labels\n",
    "        \"\"\"\n",
    "        image_names = []\n",
    "        labels = []\n",
    "\n",
    "        with open(image_list_file, \"r\") as f:\n",
    "            csvReader = csv.reader(f)\n",
    "            next(csvReader, None)\n",
    "            k=0\n",
    "            for line in csvReader:\n",
    "                k+=1\n",
    "                image_name= line[1]\n",
    "                label = line[5:]\n",
    "                \n",
    "                for i in range(14):\n",
    "                    label[i] = float(label[i])\n",
    "                \n",
    "                image_names.append(image_name)\n",
    "                labels.append(label)\n",
    "\n",
    "        self.image_names = image_names\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Take the index of item and returns the image and its labels\"\"\"\n",
    "        \n",
    "        image_name = self.image_names[index]\n",
    "        image = Image.open(image_name).convert('RGB')\n",
    "        label = self.labels[index]\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return (image, torch.FloatTensor(label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.image_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.3 ms\n"
     ]
    }
   ],
   "source": [
    "#TRANSFORM DATA\n",
    "\n",
    "normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "transformList = []\n",
    "transformList.append(transforms.RandomResizedCrop(imgtransCrop))\n",
    "transformList.append(transforms.RandomHorizontalFlip())\n",
    "transformList.append(transforms.ToTensor())\n",
    "transformList.append(normalize)      \n",
    "transformSequence=transforms.Compose(transformList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Simple Train Dataset - 10000\n",
      "Size of Train Dataset - 368945\n",
      "time: 2.92 s\n"
     ]
    }
   ],
   "source": [
    "#LOAD DATASET\n",
    "\n",
    "datasetSampleTrain = CheXpertDataSet(pathFileSampleTrain, transformSequence, policy = 'ones')\n",
    "datasetTrain = CheXpertDataSet(pathFileTrain ,transformSequence, policy = 'ones')\n",
    "datasetValid = CheXpertDataSet(pathFileValid, transformSequence)\n",
    "\n",
    "print ('Size of Simple Train Dataset - {}'.format(len(datasetSampleTrain)))\n",
    "print ('Size of Train Dataset - {}'.format(len(datasetTrain)))\n",
    "\n",
    "dataLoaderSampleTrain = DataLoader(dataset=datasetSampleTrain, batch_size=trBatchSize, shuffle=True, num_workers=0, pin_memory = True)\n",
    "dataLoaderTrain = DataLoader(dataset=datasetTrain, batch_size=trBatchSize, shuffle=True,  num_workers=0, pin_memory = True)\n",
    "dataLoaderVal = DataLoader(dataset=datasetValid, batch_size=trBatchSize, shuffle=False, num_workers=0, pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.97 ms\n"
     ]
    }
   ],
   "source": [
    "class CheXpertTrainer():\n",
    "\n",
    "    def train (model, dataLoaderTrain, dataLoaderVal, nnClassCount, trMaxEpoch, launchTimestamp, checkpoint):\n",
    "        \n",
    "        #SETTINGS: OPTIMIZER & SCHEDULER\n",
    "        optimizer = optim.Adam (model.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-5)\n",
    "                \n",
    "        #SETTINGS: LOSS\n",
    "        loss = torch.nn.BCELoss(size_average = True)\n",
    "        \n",
    "        #LOAD CHECKPOINT \n",
    "        if checkpoint != None and use_gpu:\n",
    "            modelCheckpoint = torch.load(checkpoint)\n",
    "            model.load_state_dict(modelCheckpoint['state_dict'])\n",
    "            optimizer.load_state_dict(modelCheckpoint['optimizer'])\n",
    "\n",
    "        \n",
    "        #TRAIN THE NETWORK\n",
    "        lossMIN = 100000\n",
    "        \n",
    "        for epochID in range(0, trMaxEpoch):\n",
    "            \n",
    "            timestampTime = time.strftime(\"%H%M%S\")\n",
    "            timestampDate = time.strftime(\"%d%m%Y\")\n",
    "            timestampSTART = timestampDate + '-' + timestampTime\n",
    "            \n",
    "            batchs, losst, losse = CheXpertTrainer.epochTrain(model, dataLoaderTrain, optimizer, trMaxEpoch, nnClassCount, loss)\n",
    "            lossVal = CheXpertTrainer.epochVal(model, dataLoaderVal, optimizer, trMaxEpoch, nnClassCount, loss)\n",
    "\n",
    "\n",
    "            timestampTime = time.strftime(\"%H%M%S\")\n",
    "            timestampDate = time.strftime(\"%d%m%Y\")\n",
    "            timestampEND = timestampDate + '-' + timestampTime\n",
    "            \n",
    "            if lossVal < lossMIN:\n",
    "                lossMIN = lossVal    \n",
    "                torch.save({'epoch': epochID + 1, 'state_dict': model.state_dict(), 'best_loss': lossMIN, 'optimizer' : optimizer.state_dict()}, 'm-epoch'+str(epochID)+'-' + launchTimestamp + '.pth.tar')\n",
    "                print ('Epoch [' + str(epochID + 1) + '] [save] [' + timestampEND + '] loss= ' + str(lossVal))\n",
    "            else:\n",
    "                print ('Epoch [' + str(epochID + 1) + '] [----] [' + timestampEND + '] loss= ' + str(lossVal))\n",
    "        \n",
    "        return (batchs, losst, losse)\n",
    "    #-------------------------------------------------------------------------------- \n",
    "       \n",
    "    def epochTrain(model, dataLoader, optimizer, epochMax, classCount, loss):\n",
    "        \n",
    "        batch = []\n",
    "        losstrain = []\n",
    "        losseval = []\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        for batchID, (varInput, target) in enumerate(dataLoaderTrain):\n",
    "            \n",
    "            varTarget = target.cuda(non_blocking = True)\n",
    "            \n",
    "            #varTarget = target.cuda()         \n",
    "            \n",
    "            print (varInput.shape)\n",
    "            varOutput = model(varInput)\n",
    "            lossvalue = loss(varOutput, varTarget)\n",
    "                       \n",
    "            optimizer.zero_grad()\n",
    "            lossvalue.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            l = lossvalue.item()\n",
    "            losstrain.append(l)\n",
    "            \n",
    "            if batchID%35==0:\n",
    "                logger.info('Batches Computed - {}'.format(batchID//35))\n",
    "                print(batchID//35, \"% batches computed\")\n",
    "                #Fill three arrays to see the evolution of the loss\n",
    "\n",
    "\n",
    "                batch.append(batchID)\n",
    "                \n",
    "                le = CheXpertTrainer.epochVal(model, dataLoaderVal, optimizer, trMaxEpoch, nnClassCount, loss).item()\n",
    "                losseval.append(le)\n",
    "                \n",
    "                logger.info('Batch ID - {}'.format(batchID))\n",
    "                logger.info('Training Loss - {}'.format(l))\n",
    "                logger.info('Validation Loss - {}'.format(le))\n",
    "                \n",
    "                print('Batch ID - {}'.format(batchID))\n",
    "                print('Training Loss - {}'.format(l))\n",
    "                print('Validation Loss - {}'.format(le))\n",
    "                \n",
    "        return (batch, losstrain, losseval)\n",
    "    \n",
    "    #-------------------------------------------------------------------------------- \n",
    "    \n",
    "    def epochVal(model, dataLoader, optimizer, epochMax, classCount, loss):\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        lossVal = 0\n",
    "        lossValNorm = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, (varInput, target) in enumerate(dataLoaderVal):\n",
    "                \n",
    "                target = target.cuda(non_blocking = True)\n",
    "                varOutput = model(varInput)\n",
    "                \n",
    "                losstensor = loss(varOutput, target)\n",
    "                lossVal += losstensor\n",
    "                lossValNorm += 1\n",
    "                \n",
    "        outLoss = lossVal / lossValNorm\n",
    "        return (outLoss)\n",
    "    \n",
    "    \n",
    "    #--------------------------------------------------------------------------------     \n",
    "     \n",
    "    #---- Computes area under ROC curve \n",
    "    #---- dataGT - ground truth data\n",
    "    #---- dataPRED - predicted data\n",
    "    #---- classCount - number of classes\n",
    "    \n",
    "    def computeAUROC (dataGT, dataPRED, classCount):\n",
    "        \n",
    "        outAUROC = []\n",
    "        \n",
    "        datanpGT = dataGT.cpu().numpy()\n",
    "        datanpPRED = dataPRED.cpu().numpy()\n",
    "        \n",
    "        for i in range(classCount):\n",
    "            try:\n",
    "                outAUROC.append(roc_auc_score(datanpGT[:, i], datanpPRED[:, i]))\n",
    "            except ValueError:\n",
    "                pass\n",
    "        return (outAUROC)\n",
    "        \n",
    "        \n",
    "    #-------------------------------------------------------------------------------- \n",
    "    \n",
    "    \n",
    "    def test(model, dataLoaderTest, nnClassCount, checkpoint, class_names):   \n",
    "        \n",
    "        cudnn.benchmark = True\n",
    "        \n",
    "        if checkpoint != None and use_gpu:\n",
    "            modelCheckpoint = torch.load(checkpoint)\n",
    "            model.load_state_dict(modelCheckpoint['state_dict'])\n",
    "\n",
    "        if use_gpu:\n",
    "            outGT = torch.FloatTensor().cuda()\n",
    "            outPRED = torch.FloatTensor().cuda()\n",
    "        else:\n",
    "            outGT = torch.FloatTensor()\n",
    "            outPRED = torch.FloatTensor()\n",
    "       \n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, (input, target) in enumerate(dataLoaderTest):\n",
    "\n",
    "                target = target.cuda()\n",
    "                outGT = torch.cat((outGT, target), 0).cuda()\n",
    "                \n",
    "                bs, c, h, w = input.size()\n",
    "                varInput = input.view(-1, c, h, w)\n",
    "            \n",
    "                out = model(varInput)\n",
    "                outPRED = torch.cat((outPRED, out), 0)\n",
    "        aurocIndividual = CheXpertTrainer.computeAUROC(outGT, outPRED, nnClassCount)\n",
    "        aurocMean = np.array(aurocIndividual).mean()\n",
    "        \n",
    "        logger.info('AUROC mean ', aurocMean)\n",
    "        print ('AUROC mean ', aurocMean)\n",
    "        \n",
    "        for i in range (0, len(aurocIndividual)):\n",
    "            print (class_names[i], ' ', aurocIndividual[i])\n",
    "        \n",
    "        return (outGT, outPRED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.51 ms\n"
     ]
    }
   ],
   "source": [
    "class DenseNet121(nn.Module):\n",
    "    \"\"\"Model modified.\n",
    "    The architecture of our model is the same as standard DenseNet121\n",
    "    except the classifier layer which has an additional sigmoid function.\n",
    "    \"\"\"\n",
    "    def __init__(self, out_size):\n",
    "        super(DenseNet121, self).__init__()\n",
    "        self.densenet121 = torchvision.models.densenet121(pretrained=True)\n",
    "        num_ftrs = self.densenet121.classifier.in_features\n",
    "        self.densenet121.classifier = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, out_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.densenet121(x)\n",
    "        return (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.47 s\n"
     ]
    }
   ],
   "source": [
    "# initialize and load the model\n",
    "\n",
    "model = DenseNet121(nnClassCount).cuda()\n",
    "model = torch.nn.DataParallel(model).cuda()\n",
    "logger.info('Initial model is loaded on to GPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 161 ms\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = torch.load('../model/model_ones_3epoch_densenet.tar')\n",
    "model_state_dict = pretrained_model['state_dict']\n",
    "torch.save(model_state_dict, '../model/model_state_dict.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): DenseNet121(\n",
       "    (densenet121): DenseNet(\n",
       "      (features): Sequential(\n",
       "        (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "        (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu0): ReLU(inplace=True)\n",
       "        (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "        (denseblock1): _DenseBlock(\n",
       "          (denselayer1): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer2): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer3): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer4): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer5): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer6): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (transition1): _Transition(\n",
       "          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        )\n",
       "        (denseblock2): _DenseBlock(\n",
       "          (denselayer1): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer2): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer3): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer4): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer5): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer6): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer7): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer8): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer9): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer10): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer11): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer12): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (transition2): _Transition(\n",
       "          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        )\n",
       "        (denseblock3): _DenseBlock(\n",
       "          (denselayer1): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer2): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer3): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer4): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer5): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer6): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer7): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer8): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer9): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer10): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer11): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer12): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer13): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer14): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer15): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer16): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer17): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer18): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer19): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer20): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer21): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer22): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer23): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer24): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (transition3): _Transition(\n",
       "          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        )\n",
       "        (denseblock4): _DenseBlock(\n",
       "          (denselayer1): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer2): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer3): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer4): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer5): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer6): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer7): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer8): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer9): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer10): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer11): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer12): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer13): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer14): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer15): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (denselayer16): _DenseLayer(\n",
       "            (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu1): ReLU(inplace=True)\n",
       "            (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu2): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (classifier): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=14, bias=True)\n",
       "        (1): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 96.4 ms\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('../model/model_state_dict.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC mean  0.5288434268265898\n",
      "No Finding   0.2638449532763988\n",
      "Enlarged Cardiomediastinum   0.4880634397311175\n",
      "Cardiomegaly   0.5929519331243469\n",
      "Lung Opacity   0.7889307467665185\n",
      "Lung Lesion   0.41107512509907096\n",
      "Edema   0.4936832092934497\n",
      "Consolidation   0.5983564084521852\n",
      "Pneumonia   0.6994563493421155\n",
      "Atelectasis   0.22334262878895578\n",
      "Pneumothorax   0.4560934485868078\n",
      "Pleural Effusion   0.6071574696458462\n",
      "Pleural Other   0.4993409835527319\n",
      "Fracture   0.42284137185648385\n",
      "Support Devices   0.8586699080562272\n",
      "time: 10min 25s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/tljh/user/lib/python3.7/logging/__init__.py\", line 1034, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"/opt/tljh/user/lib/python3.7/logging/__init__.py\", line 880, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/opt/tljh/user/lib/python3.7/logging/__init__.py\", line 619, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"/opt/tljh/user/lib/python3.7/logging/__init__.py\", line 380, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"/opt/tljh/user/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/opt/tljh/user/lib/python3.7/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/traitlets/config/application.py\", line 664, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 583, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 132, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/tljh/user/lib/python3.7/asyncio/base_events.py\", line 539, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/tljh/user/lib/python3.7/asyncio/base_events.py\", line 1775, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/tljh/user/lib/python3.7/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/tornado/gen.py\", line 1233, in inner\n",
      "    self.run()\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/tornado/gen.py\", line 1147, in run\n",
      "    yielded = self.gen.send(value)\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 381, in dispatch_queue\n",
      "    yield self.process_one()\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/tornado/gen.py\", line 346, in wrapper\n",
      "    runner = Runner(result, future, yielded)\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/tornado/gen.py\", line 1080, in __init__\n",
      "    self.run()\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/tornado/gen.py\", line 1147, in run\n",
      "    yielded = self.gen.send(value)\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 365, in process_one\n",
      "    yield gen.maybe_future(dispatch(*args))\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/tornado/gen.py\", line 326, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n",
      "    yield gen.maybe_future(handler(stream, idents, msg))\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/tornado/gen.py\", line 326, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 545, in execute_request\n",
      "    user_expressions, allow_stdin,\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/tornado/gen.py\", line 326, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 300, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2858, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2886, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3063, in run_cell_async\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3254, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-17-363c538dcbf7>\", line 1, in <module>\n",
      "    outGT, outPRED = CheXpertTrainer.test(model = model, dataLoaderTest = dataLoaderSampleTrain, nnClassCount = nnClassCount, class_names = class_names, checkpoint = None)\n",
      "  File \"<ipython-input-12-ff2ad4d240f5>\", line 170, in test\n",
      "    logger.info('AUROC mean ', aurocMean)\n",
      "Message: 'AUROC mean '\n",
      "Arguments: (0.5288434268265898,)\n"
     ]
    }
   ],
   "source": [
    "outGT, outPRED = CheXpertTrainer.test(model = model, dataLoaderTest = dataLoaderSampleTrain, nnClassCount = nnClassCount, class_names = class_names, checkpoint = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 18.8 ms\n"
     ]
    }
   ],
   "source": [
    "outPRED = outPRED.cpu().detach().numpy()\n",
    "outPRED = np.array(outPRED)\n",
    "\n",
    "sample_train_data['pred_No Finding'] = np.round(outPRED[:,0])\n",
    "sample_train_data['pred_Enlarged Cardiomediastinum'] = np.round(outPRED[:,1])\n",
    "sample_train_data['pred_Cardiomegaly'] = np.round(outPRED[:,2])\n",
    "sample_train_data['pred_Lung Opacity'] = np.round(outPRED[:,3])\n",
    "sample_train_data['pred_Lung Lesion'] = np.round(outPRED[:,4])\n",
    "sample_train_data['pred_Edema'] = np.round(outPRED[:,5])\n",
    "sample_train_data['pred_Consolidation'] = np.round(outPRED[:,6])\n",
    "sample_train_data['pred_Pneumonia'] = np.round(outPRED[:,7])\n",
    "sample_train_data['pred_Atelectasis'] = np.round(outPRED[:,8])\n",
    "sample_train_data['pred_Pneumothorax'] = np.round(outPRED[:,9])\n",
    "sample_train_data['pred_Pleural Effusion'] = np.round(outPRED[:,10])\n",
    "sample_train_data['pred_Pleural Other'] = np.round(outPRED[:,11])\n",
    "sample_train_data['pred_Fracture'] = np.round(outPRED[:,12])\n",
    "sample_train_data['pred_Support Devices'] = np.round(outPRED[:,13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.69 ms\n"
     ]
    }
   ],
   "source": [
    "len(sample_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.22 ms\n"
     ]
    }
   ],
   "source": [
    "'pred_No Finding' in sample_train_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo List\n",
    "\n",
    "- Train few shot learning model on every class with 100 images.\n",
    "    - Incrementally build the few shot learning model on all the 14 conditions.\n",
    "- Validate the results on the all images (except the 10 images).\n",
    "- Compare the results with the prior results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 750 µs\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    'train_size' : 100,\n",
    "    'test_size' : 100,\n",
    "    'out_size' : 128, # should be less than 512\n",
    "    'loss_margin' : 0.2,\n",
    "    'loss_p' : 2,\n",
    "    'batch_size' : 4 # do not change this\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 20.9 ms\n"
     ]
    }
   ],
   "source": [
    "class DenseNet121ToDense(nn.Module):\n",
    "    '''\n",
    "    The architecture of the densenet121 is copied and the final classifier layer is removed.\n",
    "    In the place of final classifier layer a dense layer is attached with PReLU activation.\n",
    "    '''\n",
    "    def __init__(self, out_size):\n",
    "        super(DenseNet121ToDense, self).__init__()\n",
    "        self.densenet121todense = model.module.densenet121\n",
    "        num_ftrs = self.densenet121todense.classifier[0].in_features\n",
    "        self.densenet121todense.classifier = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, 512),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(512, out_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_1, x_2, x_3):\n",
    "        x_1 = self.densenet121todense(x_1)\n",
    "        x_2 = self.densenet121todense(x_2)\n",
    "        x_3 = self.densenet121todense(x_3)\n",
    "        return (x_1, x_2, x_3)\n",
    "    \n",
    "ref_model = DenseNet121ToDense(args['out_size']).cuda()\n",
    "ref_model = torch.nn.DataParallel(ref_model).cuda()\n",
    "logger.info('FSL model is loaded on to GPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.09 ms\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, TensorDataset\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def make_train_triplets(pathology, sample_train_data, n):\n",
    "    '''\n",
    "    Returns 'n' triplet images for training data for a given pathology.\n",
    "    First Image - Any random image (with atleast 40 images which has the pathology)\n",
    "    Second Image - An image which is positive for the pathology.\n",
    "    Third Image - An image which is negative for the pathology.\n",
    "    '''\n",
    "    \n",
    "    actual_path = pathology\n",
    "    pred_path = 'pred_'+pathology\n",
    "    false_negative = sample_train_data[(sample_train_data[actual_path] == 1.0) & (sample_train_data[pred_path] == 0.0)]\n",
    "    false_positive = sample_train_data[(sample_train_data[actual_path] == 0.0) & (sample_train_data[pred_path] == 1.0)]\n",
    "    print ('False Negatives - {} False Positives - {}'.format(len(false_negative), len(false_positive)))\n",
    "\n",
    "    positive = sample_train_data[sample_train_data[actual_path] == 1.0]\n",
    "    negative = sample_train_data[sample_train_data[actual_path] == 0.0]\n",
    "    \n",
    "    images = []\n",
    "    checking_label = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        if (len(false_negative.index) > 0 and len(false_positive.index) > 0):\n",
    "            choose = random.choice(['a','b'])    \n",
    "            if (choose == 'a'):\n",
    "                first_image_index = random.choice(false_negative.index)\n",
    "                checking_label.append(-1)\n",
    "            elif (choose == 'b'):\n",
    "                first_image_index = random.choice(false_positive.index)\n",
    "                checking_label.append(1)\n",
    "        else:\n",
    "            if (len(false_negative.index) > 0):\n",
    "                first_image_index = random.choice(false_negative.index)\n",
    "                checking_label.append(-1)\n",
    "            elif (len(false_positive.index) > 0):\n",
    "                first_image_index = random.choice(false_positive.index)\n",
    "                checking_label.append(1)\n",
    "            \n",
    "        second_image_index = random.choice(positive.index)\n",
    "        third_image_index = random.choice(negative.index)\n",
    "\n",
    "        # Convert the indices to images\n",
    "        first_image = sample_train_data['image_path'][first_image_index]\n",
    "        first_image = Image.open(first_image).convert('RGB')\n",
    "        first_image = transformSequence(first_image)\n",
    "        \n",
    "        second_image = sample_train_data['image_path'][second_image_index]\n",
    "        second_image = Image.open(second_image).convert('RGB')\n",
    "        second_image = transformSequence(second_image)\n",
    "        \n",
    "        third_image = sample_train_data['image_path'][third_image_index]\n",
    "        third_image = Image.open(third_image).convert('RGB')\n",
    "        third_image = transformSequence(third_image)\n",
    "        \n",
    "        images += [[first_image, second_image, third_image]]\n",
    "        \n",
    "    return (images, checking_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.96 ms\n"
     ]
    }
   ],
   "source": [
    "def make_test_triplets(pathology, sample_train_data):\n",
    "    '''\n",
    "    - Returns three images (so the name triplets)\n",
    "    - First Image : An Inference failed image. (Inference failures can appear in the form of false negatives and \n",
    "    false postives)\n",
    "    - Second Image: An image which is positive for the pathology.\n",
    "    - Third Image: An image which is negative for the pathology.\n",
    "    '''\n",
    "    \n",
    "    actual_path = pathology\n",
    "    pred_path = 'pred_'+pathology\n",
    "    false_negative = sample_train_data[(sample_train_data[actual_path] == 1.0) & (sample_train_data[pred_path] == 0.0)]\n",
    "    false_positive = sample_train_data[(sample_train_data[actual_path] == 0.0) & (sample_train_data[pred_path] == 1.0)]\n",
    "    print ('False Negatives - {} False Positives - {}'.format(len(false_negative), len(false_positive)))\n",
    "\n",
    "    positive = sample_train_data[sample_train_data[actual_path] == 1.0]\n",
    "    negative = sample_train_data[sample_train_data[actual_path] == 0.0]\n",
    "    \n",
    "    images = []\n",
    "    checking_label = []\n",
    "    \n",
    "    false_inference_index = false_negative.index.union(false_positive.index)\n",
    "    for index in false_inference_index:\n",
    "        if index in false_negative.index:\n",
    "            checking_label.append(-1)\n",
    "        elif index in false_positive.index:\n",
    "            checking_label.append(1)\n",
    "            \n",
    "        first_image_index  = index    \n",
    "        second_image_index = random.choice(positive.index)\n",
    "        third_image_index = random.choice(negative.index)\n",
    "\n",
    "        # Convert the indices to images\n",
    "        first_image = sample_train_data['image_path'][first_image_index]\n",
    "        first_image = Image.open(first_image).convert('RGB')\n",
    "        first_image = transformSequence(first_image)\n",
    "\n",
    "        second_image = sample_train_data['image_path'][second_image_index]\n",
    "        second_image = Image.open(second_image).convert('RGB')\n",
    "        second_image = transformSequence(second_image)\n",
    "\n",
    "        third_image = sample_train_data['image_path'][third_image_index]\n",
    "        third_image = Image.open(third_image).convert('RGB')\n",
    "        third_image = transformSequence(third_image)\n",
    "\n",
    "        images += [[first_image, second_image, third_image]]\n",
    "        \n",
    "    return (images, checking_label, false_inference_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.68 ms\n"
     ]
    }
   ],
   "source": [
    "def ref_train_image_triplets(images, checking_label):\n",
    "    first_images = [i[0] for i in images]\n",
    "    second_images = [i[1] for i in images]\n",
    "    third_images = [i[2] for i in images]\n",
    "\n",
    "    first_images = torch.stack(first_images)\n",
    "    second_images = torch.stack(second_images)\n",
    "    third_images = torch.stack(third_images)\n",
    "    \n",
    "    target = torch.Tensor(checking_label).int()    \n",
    "    return (first_images, second_images, third_images, target)\n",
    "\n",
    "\n",
    "def ref_test_image_triplets(test_images, test_checking_label):\n",
    "    test_first_images = [i[0] for i in test_images]\n",
    "    test_second_images = [i[1] for i in test_images]\n",
    "    test_third_images = [i[2] for i in test_images]\n",
    "    \n",
    "    test_first_images = torch.stack(test_first_images)\n",
    "    test_second_images = torch.stack(test_second_images)\n",
    "    test_third_images = torch.stack(test_third_images)\n",
    "    \n",
    "    test_target = torch.Tensor(test_checking_label).int()\n",
    "    return (test_first_images, test_second_images, test_third_images, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7.55 ms\n"
     ]
    }
   ],
   "source": [
    "def triplet_distance(anchor, positive, negative):\n",
    "    '''\n",
    "    The function calculates the distance between anchor, positive and anchor, negative images. The difference \n",
    "    between the pos_distance, neg_distance was calculated and tuned according to the checking_label.\n",
    "    '''\n",
    "    pos_distance = F.pairwise_distance(anchor, positive, 2)\n",
    "    neg_distance = F.pairwise_distance(anchor, negative, 2)\n",
    "    return (pos_distance, neg_distance)\n",
    "\n",
    "def predictive_value(conf_matrix):\n",
    "    '''\n",
    "    This function returns the positive predictive value and negative predictive value of a classifier.\n",
    "    Parameters:\n",
    "    \n",
    "    1. conf_matrix : A Confusion matrix from the result of a classifier.\n",
    "    '''\n",
    "    ppv = (conf_matrix[0][0]/(conf_matrix[0][0] + conf_matrix[0][1]))*100\n",
    "    npv = (conf_matrix[1][0]/(conf_matrix[1][0] + conf_matrix[1][1]))*100\n",
    "    return (ppv, npv)\n",
    "\n",
    "def train_and_eval(ref_model, train_dataloader, test_dataloader):\n",
    "    for epoch in trange(5):\n",
    "    \n",
    "        tr_loss = 0\n",
    "        nb_tr_steps = 0\n",
    "        ref_model.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            anchor, positive, negative, target = batch[0], batch[1], batch[2], batch[3]\n",
    "            anchor, positive, negative, target = Variable(anchor), Variable(positive), Variable(negative), Variable(target)\n",
    "\n",
    "            bs, c, h, w = anchor.size()\n",
    "            anchor_input = anchor.view(-1, c, h, w)\n",
    "            bs, c, h, w = positive.size()\n",
    "            positive_input = positive.view(-1, c, h, w)\n",
    "            bs, c, h, w = negative.size()\n",
    "            negative_input = negative.view(-1, c, h, w)\n",
    "\n",
    "            E1, E2, E3 = ref_model(anchor_input, positive_input, negative_input)\n",
    "            dist_E1_E2, dist_E1_E3 = triplet_distance(E1, E2, E3)\n",
    "\n",
    "            target = target.cuda()\n",
    "            loss = criterion(dist_E1_E2, dist_E1_E3, target)\n",
    "            tr_loss += loss\n",
    "            nb_tr_steps += 1\n",
    "\n",
    "            torch.autograd.set_detect_anomaly(True)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph = True)\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "\n",
    "        pred_list = []\n",
    "        with torch.no_grad():\n",
    "            ref_model.eval()\n",
    "            for step, batch in enumerate(test_dataloader):\n",
    "                anchor, positive, negative, target = batch[0], batch[1], batch[2], batch[3]\n",
    "                anchor, positive, negative, target = Variable(anchor), Variable(positive), Variable(negative), Variable(target)\n",
    "\n",
    "                bs, c, h, w = anchor.size()\n",
    "                anchor_input = anchor.view(-1, c, h, w)\n",
    "                bs, c, h, w = positive.size()\n",
    "                positive_input = positive.view(-1, c, h, w)\n",
    "                bs, c, h, w = negative.size()\n",
    "                negative_input = negative.view(-1, c, h, w)\n",
    "\n",
    "                E1, E2, E3 = ref_model(anchor_input, positive_input, negative_input)\n",
    "                dist_E1_E2, dist_E1_E3 = triplet_distance(E1, E2, E3)\n",
    "\n",
    "                for i in range(len(dist_E1_E2)):\n",
    "                    if (dist_E1_E2[i] > dist_E1_E3[i]):\n",
    "                        pred_list.append(1)\n",
    "                    else:\n",
    "                        pred_list.append(0)\n",
    "\n",
    "        mod_test_target = []\n",
    "        for i in test_target:\n",
    "            if i == -1:\n",
    "                mod_test_target.append(1)\n",
    "            else:\n",
    "                mod_test_target.append(0)\n",
    "    \n",
    "    return (mod_test_target, pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Name : No Finding\n",
      "False Negatives - 3797 False Positives - 607\n",
      "False Negatives - 3797 False Positives - 607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1981429159641266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 1/5 [07:07<28:30, 427.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10084587335586548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [14:15<21:22, 427.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.018712429329752922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [21:15<14:11, 425.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.007266358472406864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [28:13<07:03, 423.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0023711128160357475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [35:08<00:00, 420.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before FSL - \n",
      "Positive Predictive Value : 89.49463482173763\n",
      "Negative Predictive Value : 89.9336807200379\n",
      "After FSL - \n",
      "Positive Predictive Value : 93.907926618207\n",
      "Negative Predictive Value : 50.33159639981052\n",
      "--------------------------------------\n",
      "Class Name : Enlarged Cardiomediastinum\n",
      "False Negatives - 565 False Positives - 0\n",
      "False Negatives - 565 False Positives - 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.17425817251205444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 1/5 [02:27<09:48, 147.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.027121998369693756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [04:55<07:22, 147.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0049117980524897575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [07:24<04:56, 148.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.001371822552755475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [09:52<02:28, 148.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0010520974174141884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [12:22<00:00, 148.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before FSL - \n",
      "Positive Predictive Value : 100.0\n",
      "Negative Predictive Value : 100.0\n",
      "After FSL - \n",
      "Positive Predictive Value : 100.0\n",
      "Negative Predictive Value : 58.93805309734513\n",
      "--------------------------------------\n",
      "Class Name : Cardiomegaly\n",
      "False Negatives - 1676 False Positives - 911\n",
      "False Negatives - 1676 False Positives - 911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1989714801311493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 1/5 [04:50<19:21, 290.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.026790782809257507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [09:40<14:31, 290.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.004064400680363178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [14:33<09:42, 291.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0013526572147384286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [19:22<04:50, 290.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 4.8074871301651e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [24:14<00:00, 290.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before FSL - \n",
      "Positive Predictive Value : 88.7752587481518\n",
      "Negative Predictive Value : 88.95966029723992\n",
      "After FSL - \n",
      "Positive Predictive Value : 94.12272055199605\n",
      "Negative Predictive Value : 53.07855626326964\n",
      "--------------------------------------\n",
      "Class Name : Lung Opacity\n",
      "False Negatives - 1511 False Positives - 2279\n",
      "False Negatives - 1511 False Positives - 2279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.20865677297115326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 1/5 [06:11<24:47, 371.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.03487783297896385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [12:26<18:37, 372.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.003476259298622608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [18:43<12:28, 374.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0007229932816699147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [24:55<06:13, 373.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0012889286736026406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [30:38<00:00, 364.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before FSL - \n",
      "Positive Predictive Value : 71.1628495508035\n",
      "Negative Predictive Value : 72.0553171196948\n",
      "After FSL - \n",
      "Positive Predictive Value : 86.03062128305706\n",
      "Negative Predictive Value : 39.77110157367668\n",
      "--------------------------------------\n",
      "Class Name : Lung Lesion\n",
      "False Negatives - 299 False Positives - 6\n",
      "False Negatives - 299 False Positives - 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.20720255374908447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 1/5 [02:07<08:30, 127.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.04196728393435478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [04:18<06:25, 128.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.003958416171371937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [06:29<04:18, 129.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0011955222580581903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [08:39<02:09, 129.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0007965067634359002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [10:50<00:00, 129.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before FSL - \n",
      "Positive Predictive Value : 99.93815070611276\n",
      "Negative Predictive Value : 100.0\n",
      "After FSL - \n",
      "Positive Predictive Value : 99.9690753530564\n",
      "Negative Predictive Value : 50.50167224080268\n",
      "--------------------------------------\n",
      "Class Name : Edema\n",
      "False Negatives - 1158 False Positives - 1720\n",
      "False Negatives - 1158 False Positives - 1720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1989484280347824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 1/5 [04:48<19:15, 288.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.024662181735038757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [09:35<14:24, 288.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.002335724188014865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [14:46<09:49, 294.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.003430748824030161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [19:47<04:56, 296.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0012464869068935513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [24:47<00:00, 297.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before FSL - \n",
      "Positive Predictive Value : 79.94402985074626\n",
      "Negative Predictive Value : 81.32022471910112\n",
      "After FSL - \n",
      "Positive Predictive Value : 89.50559701492537\n",
      "Negative Predictive Value : 50.28089887640449\n",
      "--------------------------------------\n",
      "Class Name : Consolidation\n",
      "False Negatives - 430 False Positives - 1\n",
      "False Negatives - 430 False Positives - 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.19818906486034393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 1/5 [02:15<09:01, 135.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.020759744569659233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [04:27<06:43, 134.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0076851570047438145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [06:39<04:27, 133.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.005488424561917782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [08:51<02:13, 133.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0027426297310739756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [11:07<00:00, 133.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before FSL - \n",
      "Positive Predictive Value : 99.98955067920585\n",
      "Negative Predictive Value : 100.0\n",
      "After FSL - \n",
      "Positive Predictive Value : 99.98955067920585\n",
      "Negative Predictive Value : 57.906976744186046\n",
      "--------------------------------------\n",
      "Class Name : Pneumonia\n",
      "False Negatives - 1376 False Positives - 52\n",
      "False Negatives - 1376 False Positives - 52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2002343088388443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 1/5 [03:20<13:21, 200.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.026331735774874687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [06:39<10:00, 200.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0032423229422420263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [09:53<06:36, 198.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [13:16<03:19, 199.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0002341357758268714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [16:41<00:00, 201.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before FSL - \n",
      "Positive Predictive Value : 99.39633155328535\n",
      "Negative Predictive Value : 99.27849927849928\n",
      "After FSL - \n",
      "Positive Predictive Value : 99.686556768052\n",
      "Negative Predictive Value : 52.308802308802306\n",
      "--------------------------------------\n",
      "Class Name : Atelectasis\n",
      "False Negatives - 1956 False Positives - 417\n",
      "False Negatives - 1956 False Positives - 417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.22322429716587067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 1/5 [04:20<17:20, 260.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0227182786911726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [08:50<13:10, 263.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0029674137476831675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [13:20<08:50, 265.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.004232614301145077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [17:53<04:27, 267.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.00038035301258787513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [22:24<00:00, 268.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before FSL - \n",
      "Positive Predictive Value : 94.7401614530777\n",
      "Negative Predictive Value : 94.4015444015444\n",
      "After FSL - \n",
      "Positive Predictive Value : 97.46468213925328\n",
      "Negative Predictive Value : 55.067567567567565\n",
      "--------------------------------------\n",
      "Class Name : Pneumothorax\n",
      "False Negatives - 405 False Positives - 12\n",
      "False Negatives - 405 False Positives - 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2052149474620819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 1/5 [02:14<08:59, 134.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.02107487991452217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [04:32<06:47, 135.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.003960717469453812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [06:52<04:33, 136.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.002325511770322919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [09:11<02:17, 137.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [11:24<00:00, 136.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before FSL - \n",
      "Positive Predictive Value : 99.87493486190724\n",
      "Negative Predictive Value : 100.0\n",
      "After FSL - \n",
      "Positive Predictive Value : 99.92704533611257\n",
      "Negative Predictive Value : 50.123456790123456\n",
      "--------------------------------------\n",
      "Class Name : Pleural Effusion\n",
      "False Negatives - 1755 False Positives - 1541\n",
      "False Negatives - 1755 False Positives - 1541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1847667694091797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 1/5 [05:28<21:53, 328.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.025365864858031273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [10:54<16:23, 327.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.005649204831570387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [16:28<10:58, 329.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0009084025514312088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [22:02<05:30, 330.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.002496852306649089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [27:25<00:00, 328.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before FSL - \n",
      "Positive Predictive Value : 80.23852269812772\n",
      "Negative Predictive Value : 79.70027247956402\n",
      "After FSL - \n",
      "Positive Predictive Value : 89.06129776865863\n",
      "Negative Predictive Value : 50.45413260672116\n",
      "--------------------------------------\n",
      "Class Name : Pleural Other\n",
      "False Negatives - 95 False Positives - 0\n",
      "False Negatives - 95 False Positives - 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.21752911806106567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 1/5 [01:56<07:45, 116.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.040139179676771164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [03:53<05:49, 116.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0030097211711108685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [05:50<03:53, 116.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0035380502231419086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [07:48<01:57, 117.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [09:46<00:00, 117.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before FSL - \n",
      "Positive Predictive Value : 100.0\n",
      "Negative Predictive Value : 100.0\n",
      "After FSL - \n",
      "Positive Predictive Value : 100.0\n",
      "Negative Predictive Value : 44.21052631578947\n",
      "--------------------------------------\n",
      "Class Name : Fracture\n",
      "False Negatives - 136 False Positives - 0\n",
      "False Negatives - 136 False Positives - 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1877029985189438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 1/5 [01:55<07:42, 115.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.022608136758208275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [03:52<05:47, 115.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0032189879566431046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [05:50<03:52, 116.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.00013588133151642978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [07:50<01:57, 117.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 7.381957090046853e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [09:50<00:00, 118.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before FSL - \n",
      "Positive Predictive Value : 100.0\n",
      "Negative Predictive Value : 100.0\n",
      "After FSL - \n",
      "Positive Predictive Value : 100.0\n",
      "Negative Predictive Value : 48.529411764705884\n",
      "--------------------------------------\n",
      "Class Name : Support Devices\n",
      "False Negatives - 1676 False Positives - 1919\n",
      "False Negatives - 1676 False Positives - 1919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.21427179872989655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 1/5 [05:46<23:07, 346.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.026318073272705078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [11:41<17:27, 349.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.002913641044870019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [17:30<11:38, 349.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0016794210532680154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [23:25<05:50, 350.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.00016746150504332036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [29:19<00:00, 351.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before FSL - \n",
      "Positive Predictive Value : 75.31515307435039\n",
      "Negative Predictive Value : 75.29200359389039\n",
      "After FSL - \n",
      "Positive Predictive Value : 86.66066375096474\n",
      "Negative Predictive Value : 41.01527403414196\n",
      "--------------------------------------\n",
      "time: 5h 57min 40s\n"
     ]
    }
   ],
   "source": [
    "for _class in class_names:\n",
    "    \n",
    "    actual_class = _class\n",
    "    pred_class = 'pred_'+_class\n",
    "    print ('Class Name : {}'.format(actual_class))\n",
    "    logger.info('Class Name : {}'.format(actual_class))\n",
    "    images, checking_label = make_train_triplets(_class, sample_train_data, 150)\n",
    "    test_images, test_checking_label, false_inference_index = make_test_triplets(_class, sample_train_data)\n",
    "    \n",
    "    first_images, second_images, third_images, target = ref_train_image_triplets(images, checking_label)\n",
    "    test_first_images, test_second_images, test_third_images, test_target = ref_test_image_triplets(test_images, test_checking_label)\n",
    "    \n",
    "    ref_train_data = TensorDataset(first_images, second_images, third_images, target)\n",
    "    train_dataloader = DataLoader(ref_train_data, batch_size = args['batch_size'], shuffle = True, num_workers = 8, pin_memory = True)\n",
    "\n",
    "    test_data = TensorDataset(test_first_images, test_second_images, test_third_images, test_target)\n",
    "    test_dataloader = DataLoader(test_data, batch_size = args['batch_size'], shuffle = False, num_workers = 8, pin_memory = True)\n",
    "\n",
    "    criterion = torch.nn.MarginRankingLoss(margin = args['loss_margin'])\n",
    "    optimizer = optim.Adam (model.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-5)\n",
    "    \n",
    "    mod_test_target, pred_list = train_and_eval(ref_model, train_dataloader, test_dataloader)\n",
    "    \n",
    "    # Calculating the ROC-AUC Scores before and after the few shot learning algorithm.\n",
    "    sample_train_data_copy_pred_class = copy.deepcopy(sample_train_data[pred_class])\n",
    "    i = 0\n",
    "    for index in false_inference_index:\n",
    "        sample_train_data_copy_pred_class[index] = pred_list[i]\n",
    "        i += 1\n",
    "        \n",
    "    sample_train_data['FSL_pred_'+_class] = sample_train_data_copy_pred_class\n",
    "    \n",
    "    print ('Before FSL - ') \n",
    "    logger.info('Before FSL - ')\n",
    "    ppv, npv = predictive_value(confusion_matrix(sample_train_data[actual_class], sample_train_data[pred_class]))\n",
    "    print ('Positive Predictive Value : {}'.format(ppv))\n",
    "    logger.info('Positive Predictive Value : {}'.format(ppv))\n",
    "    print ('Negative Predictive Value : {}'.format(npv))\n",
    "    logger.info('Negative Predictive Value : {}'.format(npv))\n",
    "    \n",
    "    print ('After FSL - ')\n",
    "    logger.info('After FSL - ')\n",
    "    ppv, npv = predictive_value(confusion_matrix(sample_train_data[actual_class], sample_train_data_copy_pred_class))\n",
    "    print ('Positive Predictive Value : {}'.format(ppv))\n",
    "    logger.info('Positive Predictive Value : {}'.format(ppv))\n",
    "    print ('Negative Predictive Value : {}'.format(npv))\n",
    "    logger.info('Negative Predictive Value : {}'.format(npv))\n",
    "    print ('--------------------------------------')\n",
    "    logger.info('--------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
